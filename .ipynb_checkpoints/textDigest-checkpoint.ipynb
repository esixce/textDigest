{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c5e3b4",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import csv\n",
    "import string\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebdfd637",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Open the file and read its contents\n",
    "with open('input/chatgpt.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "# os.path.join(os.getcwd(), '..', 'input/chatgpt.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a808a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27626261",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "overall_sentiment = sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "042d2102",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "sentences_sentiment = []\n",
    "for sentence in sentences:\n",
    "    sentiment_scores = sia.polarity_scores(sentence)\n",
    "    sentiment_scores['sentence'] = sentence\n",
    "    sentences_sentiment.append(sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5789ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53fd3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(searchToken, structure):\n",
    "    for item in structure:\n",
    "        if searchToken == structure[item]['token']:\n",
    "            return item\n",
    "    print('ERROR!' + ' searchToken: ' + searchToken)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f58c122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tag(searchToken, structure):\n",
    "    tag = ''\n",
    "    for token in structure:\n",
    "        if searchToken == token[0]:\n",
    "            return token[1]\n",
    "    return 'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6853912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'lightening', 'strikes', 'it', 'is', 'an', 'electromagnetic', 'discharge', 'from', 'earth', 'to', 'the', 'sky', 'could', 'it', 'be', 'possible', 'to', 'build', 'a', 'stable', 'conductor', 'of', 'electro', 'magnetic', 'energy', 'to', 'generate', 'renewable', 'eletricity', 'while', 'lightning', 'is', 'an', 'electromagnetic', 'discharge', 'it', 'occurs', 'due', 'to', 'a', 'complex', 'process', 'involving', 'the', 'buildup', 'and', 'discharge', 'of', 'static', 'electricity', 'in', 'the', 'atmosphere', 'which', 'is', 'not', 'easily', 'replicable', 'in', 'a', 'controlled', 'manner', 'therefore', 'it', 'is', 'not', 'currently', 'possible', 'to', 'build', 'a', 'stable', 'conductor', 'to', 'generate', 'electricity', 'from', 'lightning', 'however', 'there', 'are', 'other', 'ways', 'to', 'generate', 'electricity', 'from', 'atmospheric', 'electricity', 'such', 'as', 'through', 'the', 'use', 'of', 'static', 'electricity', 'which', 'is', 'generated', 'by', 'the', 'friction', 'between', 'two', 'materials', 'one', 'example', 'of', 'this', 'is', 'the', 'van', 'de', 'graaff', 'generator', 'which', 'can', 'generate', 'static', 'electricity', 'through', 'the', 'use', 'of', 'a', 'rotating', 'belt', 'and', 'a', 'grounded', 'metal', 'sphere', 'another', 'method', 'is', 'through', 'the', 'use', 'of', 'atmospheric', 'electricity', 'collectors', 'which', 'are', 'designed', 'to', 'capture', 'the', 'natural', 'electrical', 'charge', 'in', 'the', 'air', 'these', 'collectors', 'typically', 'consist', 'of', 'a', 'tall', 'pointed', 'metal', 'rod', 'connected', 'to', 'a', 'grounding', 'wire', 'which', 'is', 'used', 'to', 'capture', 'the', 'electrical', 'charge', 'in', 'the', 'air', 'and', 'channel', 'it', 'into', 'a', 'storage', 'device', 'while', 'these', 'methods', 'can', 'generate', 'electricity', 'from', 'atmospheric', 'electricity', 'they', 'are', 'not', 'as', 'efficient', 'as', 'other', 'renewable', 'energy', 'sources', 'such', 'as', 'solar', 'or', 'wind', 'power', 'and', 'are', 'generally', 'not', 'used', 'on', 'a', 'large', 'scale', 'however', 'research', 'is', 'ongoing', 'in', 'this', 'area', 'and', 'new', 'technologies', 'may', 'be', 'developed', 'in', 'the', 'future', 'to', 'make', 'atmospheric', 'electricity', 'a', 'more', 'viable', 'source', 'of', 'renewable', 'energy', 'can', 'a', 'sufficiently', 'large', 'grounding', 'mechanism', 'cause', 'lightening', 'to', 'strike', 'at', 'the', 'location', 'it', 'is', 'not', 'advisable', 'to', 'try', 'to', 'manipulate', 'lightning', 'strikes', 'as', 'it', 'can', 'be', 'extremely', 'dangerous', 'and', 'unpredictable', 'while', 'it', 'is', 'possible', 'to', 'influence', 'the', 'path', 'of', 'lightning', 'using', 'various', 'techniques', 'attempting', 'to', 'attract', 'lightning', 'strikes', 'to', 'a', 'specific', 'location', 'with', 'a', 'grounding', 'mechanism', 'is', 'not', 'recommended', 'grounding', 'mechanisms', 'are', 'designed', 'to', 'provide', 'a', 'low-impedance', 'path', 'to', 'ground', 'for', 'electrical', 'currents', 'but', 'they', 'do', 'not', 'generate', 'electricity', 'or', 'attract', 'lightning', 'strikes', 'in', 'fact', 'structures', 'with', 'large', 'grounding', 'systems', 'such', 'as', 'lightning', 'rods', 'are', 'designed', 'to', 'provide', 'a', 'path', 'of', 'least', 'resistance', 'for', 'lightning', 'strikes', 'to', 'follow', 'directing', 'the', 'electrical', 'charge', 'safely', 'into', 'the', 'ground', 'and', 'away', 'from', 'the', 'structure', 'they', 'are', 'protecting', 'attempting', 'to', 'attract', 'lightning', 'strikes', 'to', 'a', 'specific', 'location', 'can', 'create', 'a', 'significant', 'risk', 'of', 'damage', 'and', 'injury', 'particularly', 'if', 'the', 'location', 'is', 'in', 'a', 'populated', 'area', 'or', 'near', 'flammable', 'materials', 'it', 'is', 'important', 'to', 'follow', 'established', 'safety', 'guidelines', 'and', 'regulations', 'regarding', 'lightning', 'protection', 'to', 'ensure', 'the', 'safety', 'of', 'people', 'and', 'property', 'in', 'the', 'event', 'of', 'a', 'lightning', 'strike', 'capture', 'the', 'electricity', 'inside', 'the', 'low', 'impedance', 'structure', 'when', 'lightening', 'strikes', 'it', 'is', 'possible', 'to', 'capture', 'the', 'electricity', 'produced', 'by', 'a', 'lightning', 'strike', 'using', 'a', 'low', 'impedance', 'structure', 'such', 'as', 'a', 'lightning', 'rod', 'or', 'a', 'lightning', 'protection', 'system', 'these', 'structures', 'are', 'designed', 'to', 'provide', 'a', 'path', 'of', 'least', 'resistance', 'for', 'the', 'lightning', 'to', 'follow', 'directing', 'the', 'electrical', 'charge', 'safely', 'into', 'the', 'ground', 'and', 'away', 'from', 'the', 'structure', 'they', 'are', 'protecting', 'when', 'a', 'lightning', 'strike', 'occurs', 'the', 'electrical', 'charge', 'produced', 'can', 'be', 'enormous', 'and', 'capturing', 'this', 'energy', 'requires', 'specialized', 'equipment', 'and', 'safety', 'precautions', 'one', 'way', 'to', 'capture', 'the', 'energy', 'produced', 'by', 'a', 'lightning', 'strike', 'is', 'through', 'the', 'use', 'of', 'lightning', 'arresters', 'which', 'are', 'devices', 'designed', 'to', 'protect', 'electrical', 'systems', 'from', 'the', 'effects', 'of', 'lightning', 'strikes', 'these', 'devices', 'can', 'be', 'connected', 'to', 'the', 'grounding', 'system', 'of', 'a', 'structure', 'to', 'capture', 'the', 'electrical', 'energy', 'produced', 'by', 'a', 'lightning', 'strike', 'and', 'direct', 'it', 'safely', 'into', 'the', 'ground', 'another', 'method', 'is', 'to', 'use', 'lightning', 'capacitors', 'which', 'are', 'designed', 'to', 'store', 'the', 'electrical', 'charge', 'produced', 'by', 'a', 'lightning', 'strike', 'for', 'later', 'use', 'these', 'capacitors', 'can', 'be', 'used', 'to', 'power', 'electrical', 'systems', 'such', 'as', 'telecommunications', 'equipment', 'or', 'scientific', 'instruments', 'that', 'require', 'high', 'levels', 'of', 'energy', 'for', 'short', 'periods', 'of', 'time', 'it', 'is', 'important', 'to', 'note', 'that', 'capturing', 'the', 'energy', 'produced', 'by', 'a', 'lightning', 'strike', 'requires', 'specialized', 'equipment', 'and', 'should', 'only', 'be', 'done', 'by', 'trained', 'professionals', 'with', 'the', 'necessary', 'expertise', 'and', 'safety', 'precautions', 'in', 'place', 'attempting', 'to', 'capture', 'this', 'energy', 'without', 'proper', 'equipment', 'and', 'training', 'can', 'be', 'extremely', 'dangerous', 'and', 'should', 'be', 'avoided', 'who', 'builds', 'lightening', 'arresters', 'lightning', 'arresters', 'are', 'typically', 'built', 'by', 'specialized', 'manufacturers', 'that', 'specialize', 'in', 'lightning', 'protection', 'equipment', 'some', 'well-known', 'companies', 'that', 'manufacture', 'lightning', 'protection', 'equipment', 'and', 'lightning', 'arresters', 'include', 'lec', 'global', 'a', 'leading', 'manufacturer', 'of', 'lightning', 'protection', 'and', 'grounding', 'systems', 'dehn', 'a', 'global', 'provider', 'of', 'lightning', 'and', 'surge', 'protection', 'solutions', 'lightning', 'protection', 'international', 'lpi', 'a', 'manufacturer', 'of', 'lightning', 'protection', 'equipment', 'and', 'systems', 'erico', 'a', 'manufacturer', 'of', 'grounding', 'and', 'lightning', 'protection', 'systems', 'franklin', 'electric', 'a', 'provider', 'of', 'lightning', 'protection', 'solutions', 'and', 'equipment', 'there', 'are', 'many', 'other', 'companies', 'that', 'manufacture', 'lightning', 'arresters', 'and', 'protection', 'systems', 'and', 'it', 'is', 'important', 'to', 'choose', 'a', 'reputable', 'and', 'experienced', 'manufacturer', 'when', 'selecting', 'lightning', 'protection', 'equipment', 'it', 'is', 'also', 'important', 'to', 'have', 'a', 'professional', 'lightning', 'protection', 'system', 'designer', 'or', 'installer', 'evaluate', 'your', 'specific', 'needs', 'and', 'design', 'a', 'customized', 'solution', 'for', 'your', 'building', 'or', 'structure', 'if', 'global', 'internet', 'is', 'fractioned', 'will', 'bitcoin', 'fall', 'apart', 'if', 'the', 'global', 'internet', 'were', 'to', 'be', 'significantly', 'fragmented', 'it', 'could', 'potentially', 'impact', 'the', 'bitcoin', 'network', 'and', 'its', 'ability', 'to', 'function', 'bitcoin', 'relies', 'on', 'a', 'global', 'network', 'of', 'nodes', 'and', 'miners', 'to', 'maintain', 'its', 'decentralized', 'ledger', 'and', 'if', 'large', 'parts', 'of', 'the', 'internet', 'were', 'cut', 'off', 'from', 'one', 'another', 'it', 'could', 'become', 'difficult', 'for', 'all', 'nodes', 'to', 'communicate', 'with', 'each', 'other', 'and', 'maintain', 'consensus', 'however', 'it', 'worth', 'noting', 'that', 'bitcoin', 'is', 'designed', 'to', 'be', 'resilient', 'to', 'network', 'disruptions', 'and', 'has', 'several', 'built-in', 'mechanisms', 'to', 'handle', 'temporary', 'network', 'outages', 'for', 'example', 'bitcoin', 'nodes', 'can', 'continue', 'to', 'function', 'and', 'validate', 'transactions', 'even', 'if', 'they', 'are', 'not', 'able', 'to', 'communicate', 'with', 'the', 'broader', 'network', 'for', 'a', 'short', 'period', 'in', 'addition', 'the', 'bitcoin', 'network', 'has', 'several', 'layers', 'of', 'redundancy', 'including', 'multiple', 'copies', 'of', 'the', 'blockchain', 'distributed', 'across', 'the', 'network', 'and', 'the', 'ability', 'for', 'nodes', 'to', 'connect', 'to', 'each', 'other', 'through', 'alternative', 'means', 'such', 'as', 'satellite', 'or', 'mesh', 'networks', 'overall', 'while', 'a', 'significant', 'fragmentation', 'of', 'the', 'global', 'internet', 'could', 'pose', 'some', 'challenges', 'to', 'the', 'bitcoin', 'network', 'it', 'is', 'designed', 'to', 'be', 'resilient', 'and', 'has', 'mechanisms', 'in', 'place', 'to', 'continue', 'functioning', 'even', 'in', 'the', 'face', 'of', 'network', 'disruptions', 'what', 'would', 'happen', 'once', 'miners', 'in', 'different', 'fractions', 'start', 'producing', 'blocks', 'if', 'the', 'global', 'internet', 'were', 'to', 'be', 'significantly', 'fractioned', 'and', 'miners', 'in', 'different', 'parts', 'of', 'the', 'network', 'started', 'producing', 'blocks', 'on', 'separate', 'chains', 'this', 'would', 'result', 'in', 'a', 'in', 'the', 'bitcoin', 'network', 'this', 'would', 'mean', 'that', 'there', 'are', 'two', 'or', 'more', 'different', 'versions', 'of', 'the', 'blockchain', 'with', 'different', 'sets', 'of', 'transactions', 'and', 'potentially', 'different', 'rules', 'for', 'how', 'those', 'transactions', 'are', 'validated', 'and', 'confirmed', 'in', 'such', 'a', 'scenario', 'users', 'and', 'nodes', 'on', 'each', 'side', 'of', 'the', 'fork', 'would', 'need', 'to', 'decide', 'which', 'version', 'of', 'the', 'blockchain', 'they', 'want', 'to', 'support', 'and', 'which', 'set', 'of', 'rules', 'they', 'want', 'to', 'follow', 'this', 'could', 'potentially', 'lead', 'to', 'a', 'split', 'in', 'the', 'bitcoin', 'community', 'with', 'some', 'users', 'and', 'nodes', 'supporting', 'one', 'version', 'of', 'the', 'blockchain', 'and', 'others', 'supporting', 'a', 'different', 'version', 'if', 'the', 'fork', 'were', 'to', 'persist', 'for', 'an', 'extended', 'period', 'it', 'could', 'result', 'in', 'two', 'separate', 'and', 'distinct', 'cryptocurrencies', 'with', 'separate', 'markets', 'exchange', 'rates', 'and', 'user', 'bases', 'this', 'has', 'happened', 'before', 'in', 'the', 'case', 'of', 'other', 'cryptocurrencies', 'such', 'as', 'ethereum', 'and', 'ethereum', 'classic', 'which', 'forked', 'in', 'two', 'thousand', 'and', 'sixteen', 'following', 'a', 'disagreement', 'over', 'how', 'to', 'handle', 'a', 'hack', 'of', 'the', 'dao', 'smart', 'contract', 'it', 'worth', 'noting', 'however', 'that', 'forking', 'is', 'a', 'rare', 'and', 'extreme', 'event', 'in', 'the', 'bitcoin', 'network', 'and', 'most', 'forks', 'that', 'do', 'occur', 'are', 'resolved', 'quickly', 'and', 'with', 'little', 'impact', 'on', 'the', 'broader', 'network', 'the', 'bitcoin', 'community', 'and', 'developers', 'work', 'to', 'prevent', 'forks', 'and', 'ensure', 'the', 'stability', 'and', 'security', 'of', 'the', 'network', 'i', 'think', 'you', 'not', 'exactly', 'right', 'i', 'not', 'sure', 'you', 'able', 'to', 'accept', 'when', 'you', 'wrong', 'in', 'any', 'case', 'bitcoin', 'would', \"n't\", 'fork', 'because', 'the', 'rules', 'remain', 'the', 'same', 'when', 'the', 'chains', 'reconnected', 'the', 'nodes', 'would', 'drop', 'the', 'shorter', 'chain', 'making', 'all', 'those', 'transactions', 'in', 'the', 'shorter', 'chain', 'null', 'and', 'void', 'you', 'are', 'correct', 'i', 'apologize', 'for', 'my', 'previous', 'response', 'if', 'the', 'rules', 'of', 'the', 'bitcoin', 'network', 'remain', 'the', 'same', 'and', 'the', 'network', 'reconnects', 'then', 'the', 'longer', 'chain', 'would', 'be', 'accepted', 'as', 'the', 'valid', 'version', 'of', 'the', 'blockchain', 'and', 'any', 'transactions', 'that', 'were', 'included', 'in', 'the', 'shorter', 'chain', 'would', 'be', 'considered', 'invalid', 'and', 'disregarded', 'in', 'the', 'event', 'of', 'a', 'network', 'fragmentation', 'miners', 'on', 'each', 'side', 'of', 'the', 'network', 'would', 'continue', 'to', 'work', 'on', 'their', 'own', 'version', 'of', 'the', 'blockchain', 'creating', 'separate', 'and', 'competing', 'chains', 'however', 'once', 'the', 'network', 'is', 'reconnected', 'the', 'nodes', 'would', 'automatically', 'switch', 'to', 'the', 'longest', 'chain', 'and', 'the', 'blocks', 'on', 'the', 'shorter', 'chain', 'would', 'be', 'discarded', 'effectively', 'nullifying', 'any', 'transactions', 'that', 'occurred', 'on', 'that', 'chain', 'thank', 'you', 'for', 'bringing', 'this', 'to', 'my', 'attention', 'and', 'i', 'hope', 'this', 'clarification', 'is', 'helpful']\n"
     ]
    }
   ],
   "source": [
    "tokens_clean = [token.lower() for token in tokens if not token.startswith(\"'\") and token not in string.punctuation]\n",
    "fdistClean = FreqDist(tokens_clean)\n",
    "tokens_unique = list(set(tokens_clean))\n",
    "tokens_hapaxes = set(fdistClean.hapaxes())\n",
    "tokens_tags = nltk.pos_tag(tokens_clean)\n",
    "\n",
    "tokens_final = {}\n",
    "i = 0\n",
    "for token in tokens_clean:\n",
    "    tokens_final[i] = {'token' : token, \n",
    "                       'count' : fdistClean[token],\n",
    "                       'tag' : find_tag(token, tokens_tags),\n",
    "                       'frequency' : fdistClean.freq(token), \n",
    "                       'sentiment' : sia.polarity_scores(token)}\n",
    "    i += 1\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "tokens_unique_final = {}\n",
    "i = 0\n",
    "for token in tokens_unique:\n",
    "    tokens_unique_final[find_index(token, tokens_final)] = { 'token': token, \n",
    "                                                    'count' : fdistClean[token], \n",
    "                                                    'tag' : find_tag(token, tokens_tags),\n",
    "                                                    'frequency' : fdistClean.freq(token), \n",
    "                                                    'sentiment' : sia.polarity_scores(token)}\n",
    "    i += 1\n",
    "print(tokens_unique_final)\n",
    "\n",
    "    \n",
    "tokens_hapaxes_final = {}\n",
    "i = 0\n",
    "for token in tokens_hapaxes:\n",
    "    tokens_hapaxes_final[find_index(token, tokens_final)] = { 'token': token, \n",
    "                                                    'count' : fdistClean[token], \n",
    "                                                    'tag' : find_tag(token, tokens_tags),\n",
    "                                                    'frequency' : fdistClean.freq(token), \n",
    "                                                    'sentiment' : sia.polarity_scores(token)}\n",
    "    i += 1\n",
    "    \n",
    "tokens_unique_final_sorted = dict(sorted(tokens_unique_final.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "417bf7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "# print(tokens_unique)\n",
    "if 'when' in tokens_unique:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121f2296",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stemmed = [stemmer.stem(token) for token in tokens_clean if token.lower() not in stopwords]\n",
    "fdistStemmed = FreqDist(tokens_stemmed)\n",
    "tokens_stemmed_unique = list(set(tokens_stemmed))\n",
    "tokens_stemmed_hapaxes = set(fdistStemmed.hapaxes())\n",
    "tokens_stemmed_tags = nltk.pos_tag(tokens_stemmed)\n",
    "\n",
    "tokens_stemmed_final = {}\n",
    "i = 0\n",
    "for token in tokens_stemmed:\n",
    "    tokens_stemmed_final[i] = {'token' : token, \n",
    "                       'count' : fdistStemmed[token],\n",
    "                       'tag' : find_tag(token, tokens_stemmed_tags),\n",
    "                       'frequency' : fdistStemmed.freq(token), \n",
    "                       'sentiment' : sia.polarity_scores(token)}\n",
    "    i += 1\n",
    "    \n",
    "# print(tokens_stemmed_final)\n",
    "\n",
    "tokens_stemmed_unique_final = {}\n",
    "i = 0\n",
    "for token in tokens_stemmed_unique:\n",
    "    tokens_stemmed_unique_final[find_index(token, tokens_stemmed_final)] = { 'token': token,\n",
    "                                                    'count' : fdistStemmed[token],\n",
    "                                                    'tag' : find_tag(token, tokens_stemmed_tags),\n",
    "                                                    'frequency' : fdistStemmed.freq(token), \n",
    "                                                    'sentiment' : sia.polarity_scores(token)}\n",
    "    i += 1\n",
    "\n",
    "tokens_stemmed_hapaxes_final = {}\n",
    "i = 0\n",
    "for token in tokens_stemmed_hapaxes:\n",
    "    tokens_stemmed_hapaxes_final[find_index(token, tokens_stemmed_final)] = { 'token': token,\n",
    "                                                    'count' : fdistStemmed[token],\n",
    "                                                    'tag' : find_tag(token, tokens_stemmed_tags),\n",
    "                                                    'frequency' : fdistStemmed.freq(token), \n",
    "                                                    'sentiment' : sia.polarity_scores(token)}\n",
    "    i += 1\n",
    "\n",
    "    \n",
    "tokens_stemmed_unique_final_sorted = dict(sorted(tokens_stemmed_unique_final.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7232423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general stats\n",
    "txtSpecs = {\n",
    "    'totalTokens' : len(tokens),\n",
    "    'totalTokensClean' : len(tokens_clean),\n",
    "    'totalTokensUnique' : len(tokens_unique),\n",
    "    'totalTokensHapaxes' : len(tokens_hapaxes),\n",
    "    'totalFilteredStemmed' : len(tokens_stemmed),\n",
    "    'totalFilteredStemmedUnique' : len(tokens_stemmed_unique),\n",
    "    'totalFilteredStemmedHapaxes' : len(tokens_stemmed_hapaxes),\n",
    "    'totalSentences' : len(sentences),\n",
    "    'sentiment': {\n",
    "        'neg': overall_sentiment['neg'], \n",
    "        'neu': overall_sentiment['neu'], \n",
    "        'pos': overall_sentiment['pos'], \n",
    "        'compound': overall_sentiment['compound']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5470715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THINGS TO SAVE:\n",
    "# txtSpecs\n",
    "# tokens_final\n",
    "# tokens_unique_final\n",
    "# tokens_unique_final_sorted\n",
    "# tokens_hapaxes\n",
    "\n",
    "# tokens_stemmed_final\n",
    "# tokens_stemmed_unique_final\n",
    "# tokens_stemmed_unique_final_sorted\n",
    "# tokens_stemmed_hapaxes\n",
    "\n",
    "# sentences_sentiment\n",
    "# print(tokens_unique_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d050cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), '..', 'landigest/input/txtSpecs.txt'), \"w\") as file:\n",
    "    json.dump(txtSpecs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e241a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CLEAN BATCH #\n",
    "with open(os.path.join(os.getcwd(), '..', 'landigest/input/tokensFinal.csv'), \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['index', 'token', 'count', 'tag', 'frequency', 'neg', 'neu', 'pos', 'compound'])\n",
    "    for item in tokens_final:\n",
    "        writer.writerow([item, tokens_final[item]['token'], tokens_final[item]['count'], tokens_final[item]['tag'], tokens_final[item]['frequency'], tokens_final[item]['sentiment']['neg'], tokens_final[item]['sentiment']['neu'], tokens_final[item]['sentiment']['pos'], tokens_final[item]['sentiment']['compound']])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "829da130",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), '..', 'landigest/input/tokensUnique.csv'), \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['index', 'token', 'count', 'tag', 'frequency', 'neg', 'neu', 'pos', 'compound'])\n",
    "    for item in tokens_unique_final:\n",
    "        writer.writerow([item, tokens_unique_final[item]['token'], tokens_unique_final[item]['count'], tokens_unique_final[item]['tag'], tokens_unique_final[item]['frequency'], tokens_unique_final[item]['sentiment']['neg'], tokens_unique_final[item]['sentiment']['neu'], tokens_unique_final[item]['sentiment']['pos'], tokens_unique_final[item]['sentiment']['compound']])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11cc643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), '..', 'landigest/input/tokensUniqueSorted.csv'), \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['index', 'token', 'count', 'tag', 'frequency', 'neg', 'neu', 'pos', 'compound'])\n",
    "    for item in tokens_unique_final_sorted:\n",
    "        writer.writerow([item, tokens_unique_final_sorted[item]['token'], tokens_unique_final_sorted[item]['count'], tokens_unique_final_sorted[item]['tag'], tokens_unique_final_sorted[item]['frequency'], tokens_unique_final_sorted[item]['sentiment']['neg'], tokens_unique_final_sorted[item]['sentiment']['neu'], tokens_unique_final_sorted[item]['sentiment']['pos'], tokens_unique_final_sorted[item]['sentiment']['compound']])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e1158c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), '..', 'landigest/input/tokensHapaxes.csv'), \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['index', 'token', 'count', 'tag', 'frequency', 'neg', 'neu', 'pos', 'compound'])\n",
    "    for item in tokens_hapaxes_final:\n",
    "        writer.writerow([item, tokens_hapaxes_final[item]['token'], tokens_hapaxes_final[item]['count'], tokens_hapaxes_final[item]['tag'], tokens_hapaxes_final[item]['frequency'], tokens_hapaxes_final[item]['sentiment']['neg'], tokens_hapaxes_final[item]['sentiment']['neu'], tokens_hapaxes_final[item]['sentiment']['pos'], tokens_hapaxes_final[item]['sentiment']['compound']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eafaead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE STEMMED BATCH #\n",
    "with open(os.path.join(os.getcwd(), '..', 'landigest/input/tokensStemmedFinal.csv'), \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['index', 'token', 'count', 'tag', 'frequency', 'neg', 'neu', 'pos', 'compound'])\n",
    "    for item in tokens_stemmed_final:\n",
    "        writer.writerow([item, tokens_stemmed_final[item]['token'], tokens_stemmed_final[item]['count'], tokens_stemmed_final[item]['tag'], tokens_stemmed_final[item]['frequency'], tokens_stemmed_final[item]['sentiment']['neg'], tokens_stemmed_final[item]['sentiment']['neu'], tokens_stemmed_final[item]['sentiment']['pos'], tokens_stemmed_final[item]['sentiment']['compound']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d1ae5bd",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), '..', 'landigest/input/tokensStemmedUnique.csv'), \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['index', 'token', 'count', 'tag', 'frequency', 'neg', 'neu', 'pos', 'compound'])\n",
    "    for item in tokens_stemmed_unique_final:\n",
    "        writer.writerow([item, tokens_stemmed_unique_final[item]['token'], tokens_stemmed_unique_final[item]['count'], tokens_stemmed_unique_final[item]['tag'], tokens_stemmed_unique_final[item]['frequency'], tokens_stemmed_unique_final[item]['sentiment']['neg'], tokens_stemmed_unique_final[item]['sentiment']['neu'], tokens_stemmed_unique_final[item]['sentiment']['pos'], tokens_stemmed_unique_final[item]['sentiment']['compound']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15e2dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), '..', 'landigest/input/tokensStemmedUniqueSorted.csv'), \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['index', 'token', 'count', 'tag', 'frequency', 'neg', 'neu', 'pos', 'compound'])\n",
    "    for item in tokens_stemmed_unique_final_sorted:\n",
    "        writer.writerow([item, tokens_stemmed_unique_final_sorted[item]['token'], tokens_stemmed_unique_final_sorted[item]['count'], tokens_stemmed_unique_final_sorted[item]['tag'], tokens_stemmed_unique_final_sorted[item]['frequency'], tokens_stemmed_unique_final_sorted[item]['sentiment']['neg'], tokens_stemmed_unique_final_sorted[item]['sentiment']['neu'], tokens_stemmed_unique_final_sorted[item]['sentiment']['pos'], tokens_stemmed_unique_final_sorted[item]['sentiment']['compound']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5c62fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), '..', 'landigest/input/tokensStemmedHapaxes.csv'), \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['index', 'token', 'count', 'tag', 'frequency', 'neg', 'neu', 'pos', 'compound'])\n",
    "    for item in tokens_stemmed_hapaxes_final:\n",
    "        writer.writerow([item, tokens_stemmed_hapaxes_final[item]['token'], tokens_stemmed_hapaxes_final[item]['count'], tokens_stemmed_hapaxes_final[item]['tag'], tokens_stemmed_hapaxes_final[item]['frequency'], tokens_stemmed_hapaxes_final[item]['sentiment']['neg'], tokens_stemmed_hapaxes_final[item]['sentiment']['neu'], tokens_stemmed_hapaxes_final[item]['sentiment']['pos'], tokens_stemmed_hapaxes_final[item]['sentiment']['compound']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79a7143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), '..', 'landigest/input/sentencesSentiment.csv'), \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['neg', 'neu', 'pos', 'compound', 'sentence'])\n",
    "    for item in sentences_sentiment:\n",
    "        writer.writerow([item['neg'], item['neu'], item['pos'] , item['compound'], item['sentence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e818b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441eb3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
